def forward(self,\n    img: Tensor,\n    return_features: bool=False,\n    use_fp16: bool=False,\n    no_output_bias: bool=False) -> Tensor:
_0 = __torch__.torch.nn.functional.affine_grid
_1 = __torch__.torch.nn.functional.grid_sample
batch_size, channels, height, width, = torch.size(img)
\n  if torch.eq(channels, 3):\n    pass\n  else:\n    ops.prim.RaiseException("AssertionError: ")\n  if use_fp16:\n    _2 = 5\n  else:\n    _2 = 6\n  x = torch.to(img, _2, False, False, None)\n  theta = torch.eye(2, 3, dtype=None, layout=None, device=ops.prim.device(x), pin_memory=None)\n  _3 = torch.select(torch.select(theta, 0, 0), 0, 2)\n  _4 = torch.select(torch.select(theta, 0, 0), 0, 0)\n  _5 = torch.div(_4, width)\n  _6 = torch.select(torch.select(theta, 0, 0), 0, 0)\n  _7 = torch.sub(_5, torch.div(_6, 299), alpha=1)\n  _8 = torch.add_(_3, _7, alpha=1)\n  _9 = torch.select(torch.select(theta, 0, 1), 0, 2)\n  _10 = torch.select(torch.select(theta, 0, 1), 0, 1)\n  _11 = torch.div(_10, height)\n  _12 = torch.select(torch.select(theta, 0, 1), 0, 1)\n  _13 = torch.sub(_11, torch.div(_12, 299), alpha=1)\n  _14 = torch.add_(_9, _13, alpha=1)\n  _15 = torch.to(theta, ops.prim.dtype(x), False, False, None)\n  theta0 = torch.repeat(torch.unsqueeze(_15, 0), [batch_size, 1, 1])\n  grid = _0(theta0, [batch_size, channels, 299, 299], False, )\n  x0 = _1(x, grid, "bilinear", "border", False, )\n  x1 = torch.sub_(x0, 128, 1)\n  x2 = torch.div_(x1, 128)\n  _16 = torch.reshape((self.layers).forward(x2, ), [-1, 2048])\n  features = torch.to(_16, 6, False, False, None)\n  if return_features:\n    _17 = features\n  else:\n    if no_output_bias:\n      logits0 = __torch__.torch.nn.functional.linear(features, self.output.weight, None, )\n      logits = logits0\n    else:\n      logits = (self.output).forward(features, )\n    _18 = __torch__.torch.nn.functional.softmax(logits, 1, 3, None, )\n    _17 = _18\n  return _17\n'